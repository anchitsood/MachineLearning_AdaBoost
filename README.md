# AdaBoost algorithm

Author: Anchit Sood;
Callsign: ElessarTelcontar;
License: GNU GPLv3;
Platform: Matlab;


-> adaboost_main.m description:

This script is a simple hands on demonstration of the Adaptive Boosting
algorithm. It generates a training set and a testing set of examples.
Each of these sets consists of points in the (x1, x2) space, and each
point has a label of +1 or -1 (meaning that each point can only be
classified one of two ways). In other words, each of these sets is a set
of points, and each point has an associated label of either +1 or -1 (yes
or no, true or false, spam or not spam, whichever makes more sense to the
reader).

Then, after generating the data sets, the script generates stumps or
'weak' decision boundaries using the train data. Each stump generated is 
the best possible stump considering the weights assigned to the points
from the previous stump (the first stump is generated from an arbitrary
equal weight assigned to all data points).

Next, the script combines the stumps according to the AdaBoost algorithm
to generate a classifier which is known to be an extremely strong
classifier.

The strength of the classifier is all well and good when stated verbally,
but we would like some metric which tests the performance of this
classifier. The last step of this script tests the classifier on both the
training set and the testing set. Some numbers are generated, comparing
the performance on both datasets.



The weights matrix contains the weights (importance) assigned to each point
in a set. The AdaBoost algorithm requires us to increase the weights of
the points which our previous stump classified wrongly. As a starting
point, we assign equal weights to all points in our training set, meaning
we do not prefer one over the other. Then, after generating each decision
stump, we check which points are classified wrongly by this stump. Now we
give more importance to such wrong classified points, and we would like
out next stump to do a better job of classifying these points. So, we
increase the weights assigned to these points, and correspondingly reduce
the weights assigned to the correctly classified points.

Here's the weight update rule after generating each stump:
Wrong point i: Dt(i) -> Dt(i)*exp(alpha)
(This is an increase in weight)
Correct point j: Dt(j) -> Dt(j)*exp(-alpha)
(This is a reduction in weight)
(Dt(k) is the weight assigned to the kth point in the trainng set)

The alpha is the correction factor after testing each point with a new
stump. It ensures that the next update of the weights matrix will be such
that half the weight will be put on correct examples, and half on the
incorrect ones.

alpha = 0.5*(ln((1/epsilon) - 1))
Thus, each stump, with its own value of epsilon, generates a new value of 
alpha, which updates the weights matrix.

Finally, since we need the weights to sum to 1, we
normalize the weights matrix, meaning we sum all its entries, and divide
each entry by this sum. This finally gives us a new Dt to use for
generating a new stump.



Generating a new stump from the training data uses the weights from
the previous stump. Each stump generated is the best possible using
the current weights, because stumpGenerator checks the total error
arising out of every possible configuration of that stump, and then
chooses the one which has the least error. The function
stumpGenerator also returns this epsilon (total error) and alpha
(weight update metric) based on its chosen stump.

Each stump generated by stumpGenerator is a 1-by-6 vector: 
Stump: [a b c d e f]

Here, the equation of the stump is: 
a*x1 + b*x2 + c > 0 or a*x1 + b*x2 + c < 0 
 
The choice of > or < in the above equation is dictated by 'd' in the
stump vector: if d = 1, then output the '>' equation, if d = -1,
output the '<' equation. The total weighted error (epsilon) from this
stump is given by the entry 'f' in the stump vector, and the value of
alpha calculated from this epsilon is given by the entry 'e'.

Remember, here, we want our stumps to only be vertical or horizontal,
which means a = 0, b = 1 and a = 1, b = 0 are the only two
permissible values of 'a' and 'b' in our stump equation/vector. The
value of 'c' actually defines the exact placement of the stump on our
grid of training points.

Provided below are the weight update rules for the 4 possible cases:
 
[Case 1]: If the stump is a vertical stump, meaning it separates
points into 'left' and 'right', and if this is a stump equation of
the form '>', then the following points are wrongly classified:
[Case 1](Wrongly classified points): (all points to the right of this
stump which have a label of -1) + (all points to the left of this
stump which have a label of +1)

[Case 2]: If the stump is a vertical stump, meaning it separates
points into 'left' and 'right', and if this is a stump equation of
the form '<', then the following points are wrongly classified:
[Case 2](Wrongly classified points): (all points to the right of this
stump which have a label of +1) + (all points to the left of this
stump which have a label of -1)

[Case 3]: If the stump is a horizontal stump, meaning it separates
points into 'up' and 'down', and if this is a stump equation of the
form '>', then the following points are wrongly classified:
[Case 3](Wrongly classified points): (all points above this stump
which have a label of -1) + (all points below this stump which have a
label of +1)

[Case 4]: If the stump is a horizontal stump, meaning it separates
points into 'up' and 'down', and if this is a stump equation of the
form '<', then the following points are wrongly classified:
[Case 4](Wrongly classified points): (all points above this stump
which have a label of +1) + (all points below this stump which have a
label of -1)

Update the weights of these points using the weight update metric,
then update the weights of the correctly classified points as well
using the same weight update metric, namely the value of alpha
associated with this stump (using the update rule defined above in
the explaination of the Dt matrix).



Now that we have all our stumps, we aim to develop our final calssifier
by combining the information from all these stumps. Here's how we go
about doing it:

We select a point from our training set, then test that point with all
our stumps. Each stump will either output a +1 or -1 for that point,
based on where the point is in comparoson to this stump (If this is a
vertical stump of the '>' form, for example, it will output a +1 for a
point which is to its right, and a -1 for a point to its left). We take
the value of this output from each stump, multiply it with the
corresponding alpha of that stump, and then finally add this number up
from all the stumps for this particular point. Finally, if this sum is
positive, we output a +1, or a -1 if the sum is negative. This output is
the final result about that point that our strong classifier gave us.

Most of the useful results have been plotted here. Some insights: As the 
number of stumps are increased, the margin should increase. Further, the
weighted error on each stump should not exceed 0.5, because that is the 
upper limit on the weighted error by virtue of our choice of weights 
update rule. Next, the training and testing errors must both go down 
as more stumps are used.

The plots demonstrate those key points. This is a general algorithm 
showing the implementation of AdaBoost. It can be used on any classifier 
that you want, for example, the Naive Bayes classifier.
